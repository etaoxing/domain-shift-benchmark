from dsb.dependencies import *
from dsb.utils import load_video, save_image

import itertools

from kitchen_shift.constants import BONUS_THRESH, OBS_ELEMENT_INDICES, OBS_ELEMENT_GOALS
from kitchen_shift.utils import get_task_info

# key is task_goal_tags
TASK_GOALS_STATE = {}  # generated by env, shared across parallel envs
TASK_GOALS_PIXEL = {}
TASK_GOALS_DEMO = {}


class GoalConditionedKitchenEnvWrapper(gym.Wrapper):
    def __init__(
        self,
        env,
        render_size=(128, 128),
        frame_size=None,  # to resize to
        use_pixels=False,
        separate_render=False,
        grayscale=False,
        elementwise_task_goal=True,
        done_on_success=False,
        reward_type=None,
        demo_reward_type=None,
        task_goals=[],
        obs_keys=('robot_qp', 'robot_qv'),
        # these use keys of obs that start with '_' to indicate not used for training agents
        with_task_goal=False,  # add obj state (so reached_goal can be called w/ pixel inputs)
        with_task_goal_id=False,
        task_goal_set_robotpos_from_demos=False,
        #
        filter_demos=None,
        num_demos_per_task=None,  # use all if None
        demo_dir=None,
        demo_tag='',
        demo_type='full',
        # randomize_demos=False,
    ):
        super().__init__(env)
        self.render_size = render_size
        self.frame_size = frame_size
        self.use_pixels = use_pixels
        self.grayscale = grayscale

        self.separate_render = separate_render  # option to render to obs if state-based

        self.obs_keys = obs_keys

        if filter_demos is None:
            self.filter_demos = None
        else:
            self.filter_demos = set(filter_demos)
        self.num_demos_per_task = num_demos_per_task
        self.demo_dir = demo_dir
        self.demo_tag = demo_tag
        self.demo_type = demo_type
        # self.randomize_demos = randomize_demos

        self.reward_type = reward_type
        self.demo_reward_type = demo_reward_type

        self.elementwise_task_goal = elementwise_task_goal
        self.done_on_success = done_on_success
        self.task_goals = task_goals
        self.with_task_goal = with_task_goal
        self.with_task_goal_id = with_task_goal_id
        self.task_goal_set_robotpos_from_demos = task_goal_set_robotpos_from_demos

        self.cache_dir = f"kitchenenv_cache/c{self.env.camera_id}_r{self.render_size[0]},{self.render_size[1]}_g{int(self.grayscale)}/"
        os.makedirs(self.cache_dir, exist_ok=True)

        self.task_goal_tags = []  # indexed by task_goal_id, same ordering as self.task_goals

        # obs_upper = 8
        obs_upper = np.inf
        obs_lower = -obs_upper

        _obs_dict = self.env._get_obs_dict()
        obs_dim = 0
        for k in self.obs_keys:
            obs_dim += _obs_dict[k].shape[0]

        if self.use_pixels:
            n_channels = 1 if self.grayscale else 3
            fs = self.frame_size if self.frame_size is not None else self.render_size
            obs_dict = dict(
                observation=gym.spaces.Box(low=obs_lower, high=obs_upper, shape=(obs_dim,)),
                achieved_goal=gym.spaces.Box(
                    low=0, high=255, shape=(n_channels,) + fs, dtype=np.uint8
                ),
                desired_goal=gym.spaces.Box(
                    low=0, high=255, shape=(n_channels,) + fs, dtype=np.uint8
                ),
            )

        else:
            goal_dim = self.env.N_DOF_OBJECT
            obs_dict = dict(
                observation=gym.spaces.Box(low=obs_lower, high=obs_upper, shape=(obs_dim,)),
                achieved_goal=gym.spaces.Box(low=obs_lower, high=obs_upper, shape=(goal_dim,)),
                desired_goal=gym.spaces.Box(low=obs_lower, high=obs_upper, shape=(goal_dim,)),
            )

            if self.separate_render:
                n_channels = 1 if self.grayscale else 3
                fs = self.frame_size if self.frame_size is not None else self.render_size
                obs_dict['render'] = gym.spaces.Box(
                    low=0,
                    high=255,
                    shape=(n_channels,) + fs,
                    dtype=np.uint8,
                )

        if self.with_task_goal_id:
            # metadata not used for training agent, used for checking if reached_goal
            obs_dict['_task_goal_id'] = gym.spaces.Box(low=0, high=255, shape=(1,), dtype=np.uint8)

        self.observation_space = gym.spaces.Dict(obs_dict)

    def observation(self, obs):
        if self.use_pixels:
            d_obs = dict(
                observation=np.concatenate([obs[k] for k in self.obs_keys]),
                achieved_goal=self.render(),
                desired_goal=TASK_GOALS_PIXEL[self.task_goal_tag].copy(),
            )
        else:
            d_obs = dict(
                observation=np.concatenate([obs[k] for k in self.obs_keys]),
                achieved_goal=obs['obj_qp'],
                desired_goal=TASK_GOALS_STATE[self.task_goal_tag][self.env.N_DOF_ROBOT :].copy(),
            )

            if self.separate_render:
                d_obs['render'] = self.render()

        if self.with_task_goal_id:
            d_obs['_task_goal_id'] = self.task_goal_id
        return d_obs

    def get_object_goaldiff(self, obj_qp):
        progress = {}

        success_obj = {}
        success_num_obj = 0
        for obj, obj_indices in OBS_ELEMENT_INDICES.items():
            # pull ground truth sim data (obs can be noisy)
            ag = obj_qp[obj_indices - self.env.N_DOF_ROBOT]
            dg = TASK_GOALS_STATE[self.task_goal_tag][obj_indices]
            p = np.linalg.norm(ag - dg)
            progress[f'_goaldiff_{obj}'] = p

            if obj in self.task_goals[self.task_goal_id]:
                s = p < BONUS_THRESH
                success_num_obj += s
                success_obj[obj] = s

        # compute this before adding anything other than _goaldiff to progress
        progress['_goaldiff_overall'] = np.sum(list(progress.values()))

        # task objected interacted in any order
        progress['success_num_obj'] = success_num_obj

        # task objects interacted with in order
        # progress['success_num_obj_ordered'] = success_num_obj_ordered
        return success_obj, progress

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        # if self.check_success:
        #     done = self._is_success(
        #         obs['obj_qp'],
        #         TASK_GOALS_STATE[self.task_goal_tag][self.env.N_DOF_ROBOT :],
        #         np.array([self.task_goal_id]),
        #     )
        # else:
        #     done = False

        # get sim state
        obj_qp = self.env.sim.data.qpos[-self.N_DOF_OBJECT :].copy()
        success_obj, progress = self.get_object_goaldiff(obj_qp)
        info.update(progress)

        if self.reward_type is None:
            reward = -1.0
        else:
            reward = self.compute_reward(success_obj, progress)

        if self.done_on_success:
            done = info['success_num_obj'] == 4
        else:
            done = False

        observation = self.observation(obs)
        return observation, reward, done, info

    def compute_reward(self, success_obj, progress):
        if self.reward_type == 'dense':
            return -progress['_goaldiff_overall']
        elif self.reward_type == 'subgoal':
            for obj, success in success_obj.items():
                if success and obj not in self.obj_completed:
                    self.obj_completed.add(obj)
                    return 1.0
            return 0.0
        elif self.reward_type == 'rlv':
            if progress['success_num_obj'] == 4:
                return 10.0
            else:
                return 0.0
        else:
            raise ValueError

    def reset(self, **kwargs):
        obs = self.env.reset(**kwargs)
        if self.reward_type == 'subgoal':
            self.obj_completed = set()
        self.task_goal_id, self.task_goal_tag = self.sample_goal()
        observation = self.observation(obs)
        return observation

    def set_task_goal_id(self, task_goal_id):
        self.task_goal_id = task_goal_id
        self.task_goal_tag = self.task_goal_tags[task_goal_id]

    def render(self, mode='rgb_array'):
        frame = self.env.render(mode, height=self.render_size[0], width=self.render_size[1])
        if mode == 'rgb_array':
            if self.frame_size is not None:
                frame = cv2.resize(frame, self.frame_size[::-1])
            if self.grayscale:
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
                frame = np.expand_dims(frame, -1)
            frame = frame.transpose((2, 0, 1))  # channel-first
        return frame

    def sample_goal(self):
        # assumes parallel envs use same set of task goals
        if isinstance(self.np_random, np.random.Generator):
            task_goal_id = self.np_random.integers(low=0, high=len(self.task_goals))
        elif isinstance(self.np_random, np.random.RandomState):
            task_goal_id = self.np_random.randint(low=0, high=len(self.task_goals))
        else:
            raise RuntimeError
        task_goal_tag = self.task_goal_tags[task_goal_id]
        return task_goal_id, task_goal_tag

    def _is_success(self, achieved_goal, desired_goal, task_goal_id=None):
        # as https://github.com/rail-berkeley/d4rl/blob/9b1eeab56d2adc5afa0541b6dd95399de9635390/d4rl/kitchen/kitchen_envs.py#L67

        if len(achieved_goal.shape) > 1:
            pass
        else:
            achieved_goal = achieved_goal[np.newaxis]
            desired_goal = desired_goal[np.newaxis]

            if task_goal_id is not None and len(task_goal_id.shape) == 0:  # HOTFIX
                task_goal_id = np.array([task_goal_id])

        if self.elementwise_task_goal:  # see figure 6, https://arxiv.org/pdf/1910.11956.pdf#page=8
            tasks_to_complete = np.array(self.task_goals)[task_goal_id.flatten()]

            success = np.zeros_like(task_goal_id, dtype=np.bool)
            for i, task in enumerate(tasks_to_complete):  # iterate over batch
                task_success = True
                ag = achieved_goal[i, ...]
                dg = desired_goal[i, ...]

                for element in task:  # iterate over elements to check
                    element_idx = OBS_ELEMENT_INDICES[element] - self.env.N_DOF_ROBOT

                    distance = np.linalg.norm(ag[element_idx] - dg[element_idx])
                    complete = distance < BONUS_THRESH
                    if not complete:
                        task_success = False
                        break

                success[i] = task_success
            return success
        else:
            d = np.linalg.norm(achieved_goal - desired_goal, axis=-1).reshape(-1, 1)
            return d < BONUS_THRESH

    def reached_goal(self, state):
        if self.with_task_goal:
            achieved_goal = state['_achieved_task_goal']
            desired_goal = state['_desired_task_goal']
        else:
            assert not self.use_pixels
            achieved_goal = state['achieved_goal']
            desired_goal = state['desired_goal']

        done = self._is_success(achieved_goal, desired_goal, state['_task_goal_id'])
        reward = -1.0 * np.ones_like(done)
        return reward, done

    def generate_task_goals(self):
        for task_goal_elements in self.task_goals:
            task_goal_tag = self.get_task_goal_tag(task_goal_elements)
            self.task_goal_tags.append(task_goal_tag)

            if task_goal_tag in TASK_GOALS_STATE:
                continue

            g = self.env.init_qpos.copy()
            for element in task_goal_elements:
                element_idx = OBS_ELEMENT_INDICES[element]
                element_goal = OBS_ELEMENT_GOALS[element]
                g[element_idx] = element_goal

            TASK_GOALS_STATE[task_goal_tag] = g

            if self.use_pixels:
                # TODO: change robot position so different than starting?
                if self.task_goal_set_robotpos_from_demos:
                    raise NotImplementedError

                g_image = self.generate_task_goal_pixels(g)

                TASK_GOALS_PIXEL[task_goal_tag] = g_image

                save_image(
                    os.path.join(self.cache_dir, f'{task_goal_tag}.png'),
                    g_image,
                    channel_first=True,
                )

    def generate_task_goal_pixels(self, g):
        self.env.reset()
        self.env.sim.data.qpos[:] = g
        self.env.sim.data.qvel[:] = self.env.init_qvel[:].copy()
        self.env.sim.forward()

        obs = self.render()
        return obs

    def get_task_goal_tag(self, task_goal_elements):
        t, _ = get_task_info(task_goal_elements)
        return t

    def get_task_goal_elements(self, task_goal_tag):
        return task_goal_tag.split('_')

    def generate_subset_tasks(self, task_goal_elements):
        if not (isinstance(task_goal_elements, list) or isinstance(task_goal_elements, tuple)):
            raise ValueError
        return itertools.chain.from_iterable(
            [
                itertools.combinations(task_goal_elements, k)
                for k in range(1, len(task_goal_elements) + 1)
            ]
        )

    def get_train_test_datasets(self):
        self.demos = self.load_demos()

        from .kitchen_env_demos_dataset import KitchenEnvDemosDataset

        train_dataset = KitchenEnvDemosDataset(
            [x for x in self.demos if x['which_set'] == 'friday'],
            #
            # metadata, should not be used for training
            with_task_goal=self.with_task_goal,
            task_goal_tags=getattr(self, 'task_goal_tags', None),
            task_goals_demo=TASK_GOALS_DEMO,
        )
        test_dataset = KitchenEnvDemosDataset(
            [x for x in self.demos if x['which_set'] == 'postcorl'],
        )
        return train_dataset, test_dataset

    def load_demos(self):
        assert self.demo_dir is not None

        demos = []
        fn_meta = f'c{self.camera_id}h{self.render_size[0]}w{self.render_size[1]}'
        frame_size = self.frame_size[::-1] if self.frame_size is not None else None
        n_demos = collections.defaultdict(int)

        dd = os.path.join(self.demo_dir, self.demo_tag, self.demo_type)

        for demo_task in os.listdir(dd):
            # exclude these demos b/c they match w/ friday_microwave_bottomknob_switch_slide
            # if demo_task == 'postcorl_microwave_bottomknob_switch_slide':
            if demo_task == 'postcorl_t-microwave,bottomknob,switch,slide':
                continue

            if self.filter_demos is not None and demo_task not in self.filter_demos:
                continue

            which_set = demo_task.split('_')[0]
            task_goal_tag = demo_task.split('_')[1]
            # task_goal_elements = task_goal_tag.split('t-')[0].split(',')

            TASK_GOALS_DEMO[task_goal_tag] = demo_task

            # grab demos
            for data_path in glob.glob(os.path.join(dd, demo_task) + '/*.pkl'):
                fn = data_path.split('/')[-1].split('.')[0]
                video_path = data_path[:-4] + f'_playback-{fn_meta}.avi'

                print(video_path)

                data = pickle.load(open(data_path, 'rb'))

                # TODO: include last timestep
                N = len(data['ctrl']) - 1

                observations = []
                for x in data['path']['observations']:
                    observations.append(np.concatenate([x[k] for k in self.obs_keys]))
                obs = dict(observation=np.stack(observations))

                if self.use_pixels:
                    video = load_video(
                        video_path,
                        frame_size=frame_size,
                        channel_first=True,
                        grayscale=self.grayscale,
                    )
                    obs['achieved_goal'] = video

                    # if len(video) != N + 1:
                    #     raise RuntimeError(
                    #         f'{video.shape}, {N}, {len(data["path"]["observations"])}'
                    #     )
                else:
                    # if len(data['path']['observations']) != N + 1:
                    #     raise RuntimeError

                    obs['achieved_goal'] = np.stack(
                        [x['obj_qp'] for x in data['path']['observations']]
                    )

                # robot_state = data['qpos'][:, : self.env.N_DOF_ROBOT].astype(np.float32)
                # obj_state = data['qpos'][:, self.env.N_DOF_ROBOT :].astype(np.float32)

                action = data['path']['actions']

                if self.demo_reward_type == 'subgoal' and task_goal_tag in set(self.task_goal_tags):
                    self.obj_completed = set()

                    self.task_goal_id = self.task_goal_tags.index(task_goal_tag)
                    self.task_goal_tag = task_goal_tag

                    reward = []
                    done = []
                    for x in data['path']['observations']:
                        success_obj, progress = self.get_object_goaldiff(x['obj_qp'])
                        r = self.compute_reward(success_obj, progress)
                        reward.append(r)

                        if progress['success_num_obj'] == 4:
                            done.append(1.0)
                        else:
                            done.append(0.0)

                    reward = np.array(reward, dtype=np.float32)
                    done = np.array(done, dtype=np.float32)

                    self.obj_completed = set()
                elif self.demo_reward_type == 'rlv':
                    reward = np.zeros(N, dtype=np.float32)
                    # use float here b/c of torch dataloader batching
                    done = np.zeros(N, dtype=np.float32)

                    reward[-1] = 10.0
                    done[-1] = 1
                else:
                    reward = -1.0 * np.ones(N, dtype=np.float32)
                    # use float here b/c of torch dataloader batching
                    done = np.zeros(N, dtype=np.float32)

                    reward[-1] = 0.0
                    done[-1] = 1

                demo = dict(
                    obs=obs,
                    action=action,
                    reward=reward,
                    done=done,
                    num_timesteps=N,
                    which_set=which_set,
                    # metadata, not used by policy
                    demo_task=demo_task,
                    task_goal_tag=task_goal_tag,
                    data_path=data_path,
                    video_path=video_path,
                )
                demos.append(demo)

                n_demos[demo_task] += 1
                if (
                    self.num_demos_per_task is not None
                    and n_demos[demo_task] >= self.num_demos_per_task
                ):
                    break

                # if len(demos) == 2: break # reduce num demos
                # break
            # break

        print(f'loaded {len(demos)} kitchen env demos')
        return demos
